{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e75f80d-2267-4f4c-8c59-d6ae7f1f3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beca7e62-0b40-4667-9bc9-3abd4b4f8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, nlayers: int):\n",
    "        super(DeepNet, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            layers.append(nn.Linear(10, 10))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba6028b6-b26f-4616-ad2f-262f88e8094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # set random seed for reproducibility\n",
    "\n",
    "input_tensor = torch.randn(32, 10)\n",
    "target = torch.randn(32, 10)\n",
    "\n",
    "model = DeepNet(50)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=100000.1)  # high learning rate to induce explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a7e2c3-54c4-4f16-b0fc-cda0d5b855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8becd3f8-6a43-43b0-bbac-7d443b75f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/100 | Loss: 0.9364947080612183 | Average Gradient Magnitude: 7.12428329886329e-24\n",
      " 2/100 | Loss: 4.16832032605563e+23 | Average Gradient Magnitude: 56631.140625\n",
      " 3/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      " 4/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      " 5/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      " 6/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      " 7/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      " 8/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      " 9/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "10/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "11/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "12/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "13/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "14/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "15/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "16/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "17/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "18/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "19/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "20/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "21/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "22/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "23/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "24/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "25/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "26/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "27/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "28/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "29/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "30/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "31/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "32/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "33/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "34/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "35/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "36/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "37/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "38/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "39/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "40/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "41/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "42/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "43/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "44/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "45/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "46/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "47/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "48/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "49/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "50/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "51/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "52/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "53/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "54/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "55/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "56/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "57/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "58/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "59/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "60/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "61/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "62/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "63/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "64/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "65/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "66/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "67/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "68/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "69/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "70/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "71/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "72/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "73/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "74/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "75/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "76/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "77/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "78/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "79/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "80/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "81/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "82/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "83/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "84/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "85/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "86/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "87/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "88/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "89/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "90/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "91/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "92/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "93/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "94/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "95/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "96/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "97/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "98/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "99/100 | Loss: nan | Average Gradient Magnitude: nan\n",
      "100/100 | Loss: nan | Average Gradient Magnitude: nan\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    # forward pass\n",
    "    outputs = model(input_tensor)\n",
    "    loss = criterion(outputs, target)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print the average gradient magnitude for the first layer\n",
    "    gradients = model.layers[0].weight.grad\n",
    "    avg_gradient_magnitude = gradients.abs().mean().item()\n",
    "    print(f\"{epoch+1:2d}/{n_epochs:2d} | Loss: {loss.item()} | Average Gradient Magnitude: {avg_gradient_magnitude}\")\n",
    "\n",
    "    # stop if gradients are exploding\n",
    "    if avg_gradient_magnitude > 1e6:\n",
    "        print(\"Gradients are exploding!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ce4f6-d5f5-42e8-b708-b75d419a5045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
