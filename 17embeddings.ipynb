{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f698cff3-4482-440c-b9e4-e91f9bde67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gensim datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c15e8-926b-451b-938a-e5d0c4b8e9a3",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "В термінах обробки природної мови та машинного навчання, embedding вказує на техніку перетворення слова або токена у вектор чисел фіксованої розмірності. Ці вектори називаються \"векторними вбудуваннями\" або \"ембедінгами\". Векторне вбудування дозволяє представляти слова у вигляді числових векторів у такому просторі, де схожі слова знаходяться близько одне до одного.\n",
    "\n",
    "Такі ембедінги можна навчати, використовуючи методи, такі як Word2Vec, GloVe (Global Vectors for Word Representation). Ці методи дозволяють моделі вивчати семантичні відносини між словами та знаходити числові представлення слів, які відображають їхню схожість та семантику.\n",
    "\n",
    "Ембедінги можуть бути використані у багатьох завданнях обробки природної мови, таких як класифікація текстів, машинний переклад, генерація текстів, аналіз настроїв та інші. Це дозволяє моделям краще розуміти текст та працювати з ним у вигляді числових даних, що полегшує їх тренування та покращує результати на різних завданнях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed611b91-c5d1-4296-9efd-8daf1d7fc652",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "\n",
    "**Word2Vec** - це техніка векторного представлення слів у вигляді числових векторів. Цей метод був розроблений Томасом Міколовим та його колегами в Google у 2013 році.\n",
    "\n",
    "Ідея *Word2Vec* полягає в тому, щоб навчити модель передбачати контекстні слова для даного слова в тексті. Це виконується шляхом знаходження числових векторів, які представляють слова, так, щоб схожі слова знаходилися близько одне до одного у векторному просторі. Зазвичай використовується два основних підходи до Word2Vec: Continuous Bag of Words (CBOW) та Skip-Gram.\n",
    "\n",
    "CBOW намагається передбачити цільове слово (центральне слово) на основі його контексту (навколишніх слів).\n",
    "\n",
    "Skip-Gram, навпаки, намагається передбачити контекстні слова на основі даного цільового слова.\n",
    "\n",
    "Отримані вектори можна використовувати для вирішення різних завдань обробки природної мови, таких як класифікація текстів, машинний переклад, аналіз настроїв та інші. Word2Vec допомагає у врахуванні семантичних відносин між словами та отриманні корисних векторних представлень слів у текстах.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec.png\" height=500 width=1000>\n",
    "</center>\n",
    "\n",
    "При роботі з векторами поширеним способом обчислення подібності (відстані між векторами) є:\n",
    "- **[euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance):**\n",
    "\n",
    "    Евклідова відстань (Euclidean distance) є мірою відстані між двома точками у просторі.\n",
    "\n",
    "    Евклідова відстань широко використовується в математиці та статистиці для вимірювання відстаней між точками у просторі різної розмірності. У контексті алгоритмів машинного навчання та кластеризації, евклідова відстань часто використовується для визначення схожості між об'єктами або для групування даних.\n",
    "\n",
    "\n",
    "  $$ d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2} $$\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/l2_distance.png\" height=200 width=400>\n",
    "</center>\n",
    "\n",
    "- **[cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity):**\n",
    "\n",
    "    Косинусна схожість (Cosine Similarity) - це міра схожості між двома невідомими векторами в просторі. Ця міра використовує кут між векторами для визначення ступеня їхньої схожості. Чим менший кут між векторами, тим більша косинусна схожість.\n",
    "\n",
    "    Косинусна схожість широко використовується в області обробки природної мови та інших задачах, де необхідно вимірювати схожість між текстовими або числовими представленнями об'єктів. Вона особливо корисна при порівнянні текстових документів, векторних представлень слів чи інших об'єктів у векторному просторі.\n",
    "\n",
    "  $$ \\textit{cosine similarity} = S_C(A, B) := \\cos( \\theta ) = \\frac{A \\cdot B}{||A|| ||B||} = \\frac{ \\sum_{i=1}^n A_i B_i}{ \\sqrt{\\sum_{i=1}^n A_i^2 \\cdot \\sum_{i=1}^n B_i^2 } } $$\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/cosine_distance.png\" height=200 width=500>\n",
    "</center>\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "**Word2Vec** це підхід до вивчення embeddings слів за допомогою неглибокої нейронної мережі. Основна ідея **Word2Vec** полягає в тому, щоб представити кожне слово як багатовимірний вектор, де положення вектора в цьому багатовимірному просторі відображає значення слова.\n",
    "\n",
    "Word2Vec — це алгоритм, який використовує неглибоку модель нейронної мережі, щоб дізнатися значення слів із великого корпусу текстів. На відміну від глибоких нейронних мереж (DNN), які мають кілька прихованих шарів, дрібні нейронні мережі мають лише один або два прихованих шари між входом і виходом. Це робить обробку оперативною та прозорою. Неглибока нейронна мережа Word2Vec може швидко розпізнавати семантичні подібності та ідентифікувати слова-синоніми за допомогою методів логістичної регресії, що робить її швидше, ніж DNN.\n",
    "\n",
    "Word2Vec приймає великий корпус тексту як вхідні дані та генерує векторний простір із сотнями вимірів. Кожному унікальному слову в корпусі присвоюється вектор у цьому просторі.\n",
    "\n",
    "Розробка Word2Vec також передбачала аналіз вивчених векторів і дослідження того, як ними можна маніпулювати за допомогою векторного аналізу. Наприклад, віднімаючи «чоловік» від «короля» та додаючи «жінка», результатом буде слово «королева», яке відображає аналогію «король для королеви, як чоловік для жінки».\n",
    "\n",
    "## Where is it used?\n",
    "\n",
    "Word2Vec використовується для створення векторних представлень слів у вигляді числових векторів у просторі низької розмірності. Ця техніка має ряд застосувань у сфері обробки природної мови (Natural Language Processing, NLP) та машинного навчання. Основні застосування Word2Vec включають:\n",
    "\n",
    "1. **Семантична подібність та аналогії слів:**\n",
    "   Word2Vec дозволяє отримувати числові вектори для слів, які відображають їхню семантичну схожість. Завдяки цьому можна вирішувати завдання типу \"король - чоловік + жінка = королева\", де вектори слів використовуються для аналогій та семантичних відношень між словами.\n",
    "\n",
    "2. **Покращення роботи з текстами:**\n",
    "   Векторні представлення слова можуть бути використані для покращення роботи з текстами, такими як класифікація текстів, кластеризація, аналіз настроїв, інформаційний пошук та інші.\n",
    "\n",
    "3. **Машинний переклад:**\n",
    "   Векторні представлення слів можуть покращити якість машинного перекладу, дозволяючи моделям краще розуміти семантику слів та фраз.\n",
    "\n",
    "4. **Рекомендації:**\n",
    "   Word2Vec може бути використаний для покращення систем рекомендацій, враховуючи семантичну схожість між предметами чи користувачами.\n",
    "\n",
    "5. **Кластеризація та категоризація текстів:**\n",
    "   Векторні представлення слов можуть служити основою для кластеризації та категоризації текстової інформації.\n",
    "\n",
    "6. **Аналіз настроїв:**\n",
    "   Вектори слів можуть бути використані для аналізу настроїв в текстах, допомагаючи визначити емоційний тон висловлення.\n",
    "\n",
    "Word2Vec дозволяє узагальнювати семантику слів і використовувати цю інформацію у різних задачах обробки природної мови та машинного навчання, що робить його потужним інструментом у цих областях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a650e-9df0-4838-a151-fe8701aecb1c",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "Передбачення наступного слова – це завдання, яке можна вирішити за допомогою *language model*. Мовна модель може взяти список слів (скажімо, два слова) і спробувати передбачити слово, яке йде за ними.\n",
    "\n",
    "На скріншоті нижче ми можемо уявити модель як ту, яка взяла ці два зелених слова (\"ти маєш\") і повернула список пропозицій (\"не\" є тим, що має найвищу ймовірність):\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/thou-shalt-_.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Ми можемо думати, що модель виглядає як цей чорний ящик:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/language_model_blackbox.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "Але на практиці модель не виводить лише одне слово. Вона фактично виводить оцінку ймовірності для всіх слів, які вона знає («словниковий запас моделі», який може коливатися від кількох тисяч до понад мільйона слів). Як приклад, клавіатурний додаток (клавіатура на будь-якому смарт-девайсі) має знайти слова з найвищими балами та представити їх користувачеві.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/language_model_blackbox_output_vector.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "З часів навчання ранніх нейронних мовних моделей ([Bengio 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)) обчислюють прогноз у три етапи:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/neural-language-model-prediction.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "Перший крок є найбільш актуальним для нас, коли ми обговорюємо embeddings. Одним із результатів навчального процесу стала ця матриця, яка містить embedding для кожного слова в нашому словниковому запасі. Під час передбачення ми просто шукаємо embedding вхідного слова та використовуємо їх для обчислення передбачення:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/neural-language-model-embedding.png\" height=400 width=800>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b8af4-951a-4339-874f-704d83912f06",
   "metadata": {},
   "source": [
    "## Language Model Training\n",
    "\n",
    "Мовні моделі мають величезну перевагу перед більшістю інших моделей машинного навчання. Ця перевага полягає в тому, що ми можемо навчити їх працювати над текстом, якого у нас є вдосталь. Подумайте про всі книги, статті, вміст Вікіпедії та інші форми текстових даних, які ми маємо. Порівняйте це з багатьма іншими моделями машинного навчання, які потребують ручних функцій і спеціально зібраних даних.\n",
    "\n",
    "Слова вбудовуються завдяки тому, що ми дивимося, поруч з якими іншими словами вони зазвичай з’являються. Механіка цього така\n",
    "1. Ми отримуємо багато текстових даних (скажімо, усі статті Вікіпедії, наприклад). потім\n",
    "2. У нас є вікно (скажімо, з трьох слів), яке ми ковзаємо по всьому цьому тексту.\n",
    "3. Ковзне вікно генерує навчальні зразки для нашої моделі\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/wikipedia-sliding-window.png\" height=600 width=1200>\n",
    "</center>\n",
    "\n",
    "Коли це вікно ковзає по тексту, ми (фактично) створюємо набір даних, який використовуємо для навчання моделі. Щоб точно побачити, як це робиться, давайте подивимося, як ковзне вікно обробляє цю фразу:\n",
    "\n",
    "\n",
    "`“Thou shalt not make a machine in the likeness of a human mind” ~Dune`\n",
    "\n",
    "Коли ми починаємо, вікно знаходиться на перших трьох словах речення:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/lm-sliding-window.png\" height=200 width=600>\n",
    "</center>\n",
    "\n",
    "Ми вважаємо перші два слова ознаками, а третє слово — міткою:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/lm-sliding-window-2.png\" height=200 width=600>\n",
    "</center>\n",
    "\n",
    "Потім ми пересуваємо наше вікно до наступної позиції та створюємо другий зразок:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/lm-sliding-window-3.png\" height=200 width=600>\n",
    "</center>\n",
    "\n",
    "І незабаром ми маємо більший набір даних про те, які слова, як правило, з’являються після різних пар слів:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/lm-sliding-window-4.png\" height=200 width=600>\n",
    "</center>\n",
    "\n",
    "На практиці моделі, як правило, навчаються, поки ми пересуваємо вікно. Але я вважаю більш зрозумілим логічно відокремити фазу «генерації набору даних» від фази навчання. Окрім підходів до моделювання мови на основі нейронних мереж, для навчання мовних моделей зазвичай використовувався метод під назвою N-grams (див: Chapter 3 of [Speech and Language Processing](http://web.stanford.edu/~jurafsky/slp3/)). Щоб побачити, як цей перехід від N-grams до нейронних моделей відображається на продуктах реального світу, [ось публікація в блозі за 2015 рік Swiftkey](https://www.microsoft.com/en-us/swiftkey?rtc=1&activetab=pivot_1%3aprimaryr2), представляючи їхню модель нейронної мережі та порівнюючи її з попередньою моделлю N-gram.\n",
    "\n",
    "N-грами (N-grams) — це секції тексту, що складаються з N послідовних елементів (найчастіше слів), які взяті разом. N вказує на кількість елементів у секції, і вони можуть бути символами, словами або навіть іншими одиницями тексту.\n",
    "\n",
    "Отже, якщо взяти текст \"ChatGPT є потужною мовною моделлю\", розбитий на біграми (2-грами), виглядатиме так:\n",
    "\n",
    "1. \"ChatGPT є\"\n",
    "2. \"є потужною\"\n",
    "3. \"потужною мовною\"\n",
    "4. \"мовною моделлю\"\n",
    "\n",
    "А якщо розглядати триграми (3-грами), то:\n",
    "\n",
    "1. \"ChatGPT є потужною\"\n",
    "2. \"є потужною мовною\"\n",
    "3. \"потужною мовною моделлю\"\n",
    "\n",
    "N-грами використовуються у багатьох областях обробки природної мови та статистиці мови. Вони можуть бути використані для аналізу тексту, побудови мовних моделей, розпізнавання мови, а також у багатьох інших застосуваннях. Моделі мови, які використовують N-грами, можуть враховувати ймовірність входження послідовностей слів або символів, що полегшує роботу з текстом та може бути використаною для автоматичного генерування тексту, корекції помилок та інших завдань.\n",
    "\n",
    "На базі своїх знань, заповніть пропуск:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/jay_was_hit_by_a_.png\" height=100 width=200>\n",
    "</center>\n",
    "\n",
    "Контекст, який наданий тут, складається з п’яти слів перед порожнім словом (і попередньою згадкою «bus»). Я впевнений, що більшість людей здогадалися б, що слово `bus` йде в порожнє поле. Але що, якщо я дам вам ще одну інформацію – слово після пропуску, це змінить вашу відповідь?\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/jay_was_hit_by_a_bus.png\" height=100 width=200>\n",
    "</center>\n",
    "\n",
    "Це повністю змінює те, що має залишитися в бланку. Слово `red` тепер, швидше за все, залишиться в порожньому місці. З цього ми дізнаємося, що слова як до, так і після певного слова мають інформаційну цінність. Виявляється, що врахування обох напрямків (слів ліворуч і праворуч від слова, яке ми вгадуємо) призводить до кращого ембедінгу слів. Давайте подивимося, як ми можемо налаштувати спосіб навчання моделі для врахування цього.\n",
    "\n",
    "Замість того, щоб шукати лише два слова перед цільовим словом, ми також можемо дивитися два слова після нього.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/continuous-bag-of-words-example.png\" height=200 width=300>\n",
    "</center>\n",
    "\n",
    "Якщо ми це зробимо, набір даних, який ми фактично створюємо та тренуємо модель, виглядатиме так:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/continuous-bag-of-words-dataset.png\" height=100 width=200>\n",
    "</center>\n",
    "\n",
    "Це називається архітектурою **Continuous Bag of Words**, яка описана в [word2vec paper](https://arxiv.org/pdf/1301.3781.pdf). Інша архітектура, яка також мала тенденцію показувати чудові результати, робить речі трохи інакше.\n",
    "\n",
    "Замість того, щоб вгадувати слово на основі його контексту (слів перед і після нього), ця інша архітектура намагається вгадати сусідні слова, використовуючи поточне слово. Ми можемо уявити, що вікно, яке воно ковзає проти навчального тексту, виглядає так:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window.png\" height=100 width=200>\n",
    "</center>\n",
    "\n",
    "Рожеві прямокутники мають різні відтінки, оскільки це ковзаюче вікно фактично створює чотири окремі зразки в нашому навчальному наборі даних:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window-samples.png\" height=200 width=400>\n",
    "</center>\n",
    "\n",
    "Цей метод називається архітектурою **skipgram**. Ми можемо візуалізувати вікно таким чином:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window-1.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Це додало б ці чотири зразки до нашого навчального набору даних:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window-2.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Потім ми пересуваємо наше вікно в наступне положення:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window-3.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Що генерує наші наступні чотири приклади:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window-4.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Кількома позиціями пізніше ми маємо ще багато прикладів:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/skipgram-sliding-window-5.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "**Skip-gram** є однією з архітектур для навчання векторних представлень слів, і вона використовується в контексті Word2Vec, розробленого в Google. Skip-gram є одним з двох підходів Word2Vec, іншим є Continuous Bag of Words (CBOW).\n",
    "\n",
    "У методі Skip-gram основна ідея полягає в тому, щоб навчити модель передбачати контекстні слова на основі даного центрального слова. Замість того, щоб передбачати центральне слово на основі контексту (як у CBOW), Skip-gram намагається передбачити контекстні слова для кожного центрального слова.\n",
    "\n",
    "Приклад:\n",
    "1. Вхід: \"The cat is on the mat.\"\n",
    "2. Центральне слово: \"on\"\n",
    "3. Контекстні слова, які модель намагається передбачити: \"The\", \"cat\", \"is\", \"the\", \"mat\".\n",
    "\n",
    "Основна ідея полягає в тому, що модель навчається генерувати ймовірності для контекстних слів на основі центрального слова. Таким чином, отримуються векторні представлення слів, які допомагають моделі розуміти семантичні відносини між словами у тексті.\n",
    "\n",
    "Метод Skip-gram часто використовується в задачах, де важливо мати точні векторні представлення слів, таких як у завданнях обробки природної мови та машинного навчання."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060dcdf-c8a6-484a-99d7-c0b741c89d90",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW)\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "У моделі CBOW мережа передбачає поточне слово на основі контексту (вікно навколишніх слів).\n",
    "\n",
    "Дано послідовність слів $w_1, w_2, ... , w_T$, Метою моделі CBOW є максимізація середньої логарифмічної ймовірності:\n",
    "\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\log p(w_t | w_{t-\\textit{window}}, ..., w_{t+\\textit{window}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85131940-cac3-4dc9-8adc-7acf2ce5ae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13808692 -0.1574803   0.12186277  0.08450437  0.11262822  0.01271443]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Визначення набору речень для навчання моделі Word2Vec.\n",
    "sentences = [\n",
    "    [\"hello\", \"world\"],        # перше речення\n",
    "    [\"hello\", \"gensim\"],       # друге речення\n",
    "    [\"hello\", \"python\"],       # третє речення\n",
    "    [\"hello\", \"word2vec\"],      # четверте речення\n",
    "    [\"gensimm\", \"hello\"]\n",
    "]\n",
    "\n",
    "# Створення та навчання моделі CBOW (Continuous Bag of Words).\n",
    "# CBOW - один із двох підходів у Word2Vec, який спробує передбачити слово на основі контексту.\n",
    "# Вказані параметри:\n",
    "# - vector_size: розмір вектору, який визначає кількість чисел, які представляють кожне слово у векторному просторі\n",
    "# - window: максимальна відстань між поточним та прогнозованим словом у тексті\n",
    "# - min_count: мінімальна кількість разів, яка показується слово в корпусі, щоб воно було включене до словника\n",
    "# - workers: кількість потоків для навчання моделі\n",
    "# - sg: вибір алгоритму. sg=0 використовує CBOW, sg=1 - Skip-gram\n",
    "cbow_model = Word2Vec(sentences, vector_size=6, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Доступ до векторів для конкретних слів.\n",
    "vector = cbow_model.wv[\"python\"]  # Отримання вектора для слова \"hello\"\n",
    "print(vector)  # Виведення вектора у консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59a26368-5eb4-4c50-90d9-93a8b62f0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00893712  0.00394052  0.08505583  0.15015455 -0.15504916 -0.11861348]\n"
     ]
    }
   ],
   "source": [
    "# Доступ до векторів для конкретних слів.\n",
    "vector = cbow_model.wv[\"hello\"]  # Отримання вектора для слова \"hello\"\n",
    "print(vector)  # Виведення вектора у консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc67fea4-ab73-40ae-8fc9-d3644d43a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10764787  0.1495498  -0.08359047 -0.06272286  0.12300841 -0.02555786]\n"
     ]
    }
   ],
   "source": [
    "# Доступ до векторів для конкретних слів.\n",
    "vector = cbow_model.wv[\"gensimm\"]  # Отримання вектора для слова \"hello\"\n",
    "print(vector)  # Виведення вектора у консоль"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f6209-859f-475e-aaa7-ab461e99fba1",
   "metadata": {},
   "source": [
    "***Note:***\n",
    "\n",
    "*Параметр vector_size у моделі Word2Vec визначає розмірність векторів, які використовуються для представлення слів у просторі векторів. Цей параметр вказує, скільки числових значень буде використано для кодування кожного слова. Зазвичай величина цього параметра обирається експериментально залежно від конкретної задачі і обсягу даних.*\n",
    "\n",
    "*Наприклад, якщо ви встановите vector_size рівним 100, кожне слово буде представлено у просторі векторів як вектор з 100 числових значень. Ці значення вивчаються під час тренування моделі таким чином, щоб вони відображали семантичні відносини між словами, тобто слова з подібним значенням в просторі будуть розташовані близько одне до одного.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c741ac-1051-4e0c-bde7-d2a586a3d83c",
   "metadata": {},
   "source": [
    "### Skip-Gram\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "Модель Skip-Gram використовує протилежний підхід порівняно з CBOW: вона передбачає навколишні слова за поточним словом.\n",
    "\n",
    "Дано послідовність слів $w_1, w_2, ..., w_T$, метою моделі Skip-Gram є максимізація середньої логарифмічної ймовірності:\n",
    "\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\le j \\le c, \\, j \\ne 0} \\log p(w_{t+j} | w_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "208a8c8a-1835-4a29-94ff-6dc8d4f43a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n",
      "  0.06458873  0.08972988 -0.05015428 -0.03763372]\n"
     ]
    }
   ],
   "source": [
    "# Створення та навчання моделі Skip-gram.\n",
    "# Skip-gram - ще один підхід у Word2Vec, який намагається передбачити контекст на основі слова.\n",
    "# Параметри аналогічні CBOW.\n",
    "skipgram_model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "vector = skipgram_model.wv[\"hello\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecf5ebd9-b8ca-49c4-8db0-aa10f9481cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07511582 -0.00930042  0.09538119 -0.07319167 -0.02333769 -0.01937741\n",
      "  0.08077437 -0.05930896  0.00045162 -0.04753734]\n"
     ]
    }
   ],
   "source": [
    "vector = skipgram_model.wv[\"python\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3eb3e-7d7f-4587-881c-364c231e4237",
   "metadata": {},
   "source": [
    "В обох випадках, $p(w_{t+j} | w_t)$ визначається за допомогою функції softmax:\n",
    "\n",
    "$$ p(w_{t+j} | w_t) = \\frac{ \\textit{exp}(v_{w_t}' v_{w_{t+j}}) }{ \\sum_{w=1}^W \\textit{exp}(v_{w_t}' v_w)} $$\n",
    "\n",
    "де $v_w$ and $v_w'$ є вхідним і вихідним векторними представленнями w, $W$ це кількість слів у словнику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2baac3f4-945a-4c47-b37b-c40acc6e275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thou shalt not make a machine in the likeness of a human mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bf4b5-ef76-49e3-b465-f2051316157d",
   "metadata": {},
   "source": [
    "## Word2Vec Training Process\n",
    "\n",
    "Перед початком процесу навчання ми попередньо обробляємо текст, на якому навчаємо модель. На цьому кроці ми визначаємо розмір нашого словникового запасу (ми назвемо це `vocab_size`, уявіть, що це, скажімо, 10 000) і які слова до нього належать.\n",
    "\n",
    "У процесі навчання Word2Vec, терміни `embedding` та `context` відіграють важливу роль:\n",
    "\n",
    "1. **Embedding:**\n",
    "   - **Векторне вбудування (embedding):** Це представлення слів у вигляді числових векторів у просторі низької розмірності. Векторне вбудування служить числовим кодуванням слова, що враховує його семантичні властивості. Кожне слово в корпусі мови отримує свій вектор, а векторні представлення навчаються так, щоб схожі слова знаходилися близько одне до одного у векторному просторі.\n",
    "\n",
    "2. **Context:**\n",
    "   - **Контекст:** У відомих методах Word2Vec, таких як Skip-gram, контекст визначається навколишніми словами, які служать для навчання векторного представлення центрального слова. Наприклад, якщо ми маємо речення \"The cat is on the mat,\" і обираємо слово \"on\" як центральне слово, то контекстні слова будуть \"The,\" \"cat,\" \"is,\" \"the,\" \"mat.\"\n",
    "\n",
    "Таким чином, весь процес навчання Word2Vec включає в себе створення векторних представлень слів (embedding) на основі їхніх контекстів. Модель навчається таким чином, щоб вектори центральних слів були схожими з векторами їхніх контекстів. У результаті отримується простір ембедінгів, де семантично схожі слова розташовані близько одне до одного.\n",
    "\n",
    "Отже, embedding та context є ключовими поняттями у процесі навчання Word2Vec, який визначає, як модель розуміє та враховує семантичні відносини між словами в тексті.\n",
    "\n",
    "На початку фази навчання ми створюємо дві матриці – матрицю **Embedding** та матрицю **Context**. Ці дві матриці мають вкладення для кожного слова в нашому словнику (тому `vocab_size` є одним із їхніх вимірів). Другий вимір — це те, якою довжиною має бути кожне вбудовування (`embedding_size`).\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-embedding-context-matrix.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "На початку процесу навчання ми ініціалізуємо ці матриці випадковими значеннями. Потім починаємо тренувальний процес. На кожному етапі навчання ми беремо один позитивний приклад і пов’язані з ним негативні приклади. Давайте візьмемо нашу першу групу:\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-training-example.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "Тепер у нас є чотири слова: вхідне слово `not` і вихідні/контекстні слова: `thou` (справжній сусід), `aaron` і `taco` (негативні приклади). Ми переходимо до пошуку їхніх ембедінгів – вхідне слово шукаємо в матриці ембедінгів. Для контекстних слів ми шукаємо матрицю **Context** (хоча обидві матриці мають ембедінги для кожного слова в нашому словнику).\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-lookup-embeddings.png\" height=400 width=800>\n",
    "</center>\n",
    "\n",
    "Потім ми беремо скалярний добуток введення з кожним із вбудованих контекстів. У кожному випадку це призведе до числа, це число вказує на подібність введення та вбудовування контексту.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-training-dot-product.png\" height=200 width=400>\n",
    "</center>\n",
    "\n",
    "Тепер нам потрібен спосіб перетворити ці бали на щось, що виглядає як імовірності – нам потрібно, щоб усі вони були позитивними та мали значення від нуля до одиниці. Це чудове завдання для [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), в [logistic operation](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-training-dot-product-sigmoid.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Тепер ми можемо розглядати вихідні дані сигмоїдних операцій як вихідні дані моделі для цих прикладів. Ви бачите, що `taco` має найвищу оцінку, а `aaron` все ще має найнижчу оцінку як до, так і після операцій сигмоподібної форми.\n",
    "\n",
    "Тепер, коли модель зробила прогноз, і бачимо, ніби у нас є фактична цільова мітка для порівняння, давайте обчислимо, яка помилка є в прогнозі моделі. Для цього ми просто віднімаємо оцінки сигмоїда з цільових міток.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-training-error.png\" height=200 width=400>\n",
    "</center>\n",
    "\n",
    "Ось і «навчальна» частина «машинного навчання». Тепер ми можемо використати цю оцінку помилки, щоб скоригувати вбудовування `not`, `thou`, `aaron` і `taco` так, щоб наступного разу, коли ми виконаємо це обчислення, результат був ближчим до цільових оцінок.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-training-update.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "На цьому навчальний крок завершено. Ми виходимо з нього з трохи кращими вкладеннями для слів, які беруть участь у цьому кроці (`not`, `thou`, `aaron` і `taco`). Тепер ми переходимо до наступного кроку (наступний позитивний зразок і пов’язані з ним негативні зразки) і повторюємо той самий процес.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-training-example-2.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Вбудовування продовжують удосконалюватися, поки ми кілька разів переглядаємо весь набір даних. Тоді ми можемо зупинити процес навчання, відкинути матрицю `Context` і використати матрицю `Embeddings` як наші попередньо навчені вбудовування для наступного завдання.\n",
    "\n",
    "\n",
    "### Window Size and Number of Negative Samples\n",
    "\n",
    "Двома ключовими гіперпараметрами в процесі навчання word2vec є розмір вікна та кількість негативних зразків.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-window-size.png\" height=200 width=400>\n",
    "</center>\n",
    "\n",
    "Різні завдання краще виконуються за допомогою різних розмірів вікон. Одна [евристика](https://youtu.be/tAxrlAVw-Tk?t=648) полягає в тому, що менші розміри вікон (2-15) призводять до вбудовування, де високі оцінки подібності між двома вбудовуваннями вказують на те, що слова *взаємозамінні* (зауважте, що антоніми часто взаємозамінні, якщо ми дивимося лише на слова, що їх оточують, наприклад, *хороший* і *поганий* часто з’являються в схожих контекстах). Більші розміри вікон (15-50, або навіть більше) призводять до вбудовування, де подібність більше вказує на *спорідненість* слів. На практиці вам часто доведеться надавати [анотації](https://youtu.be/ao52o9l6KGw?t=287), які скеровують процес вбудовування. Розмір вікна Gensim за замовчуванням становить 5 (п’ять слів до та п’ять слів після введеного слова, на додаток до самого вхідного слова).\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/word2vec-negative-samples.png\" height=300 width=600>\n",
    "</center>\n",
    "\n",
    "Ще одним фактором тренувального процесу є кількість негативних проб. Оригінальний документ передбачає 5-20 як хорошу кількість негативних зразків. У ньому також зазначено, що 2-5 здається достатнім, якщо у вас достатньо великий набір даних. Gensim за замовчуванням становить 5 негативних зразків."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fcfcd-1b9b-479e-be0b-6fb78fed337f",
   "metadata": {},
   "source": [
    "## Implementing Word2Vec in PyTorch\n",
    "\n",
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5ca907b-9c3f-480a-a780-19efe08ee36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6721e0c-e989-41cc-9ce5-0de27c1f857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'easily': 0,\n",
       " 'fox': 1,\n",
       " 'jumps': 2,\n",
       " 'his': 3,\n",
       " 'overtakes': 4,\n",
       " 'jumped': 5,\n",
       " 'lazy': 6,\n",
       " 'brown': 7,\n",
       " 'the': 8,\n",
       " 'fast': 9,\n",
       " 'competitor': 10,\n",
       " 'quick': 11,\n",
       " 'over': 12,\n",
       " 'runner': 13,\n",
       " 'overtook': 14,\n",
       " 'dog': 15}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Використання простих даних\n",
    "data = [\n",
    "    (\"the quick brown fox jumped over the lazy dog\", \"jumps\"),  # перше речення та його мітка\n",
    "    (\"the fast runner easily overtook his competitor\", \"overtakes\"),  # друге речення та його мітка\n",
    "    # Додавання додаткових речень, якщо потрібно\n",
    "]\n",
    "\n",
    "# Створення словника\n",
    "vocab = set()  # Ініціалізація пустого множинного словника\n",
    "for context, target in data:\n",
    "    vocab.update(context.split())  # Додавання слів з контекстів у словник\n",
    "    vocab.update([target])          # Додавання цільових слів у словник\n",
    "vocab = list(vocab)  # Перетворення множини у список, щоб мати можливість індексації\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}  # Створення словника індексів для кожного слова у словнику\n",
    "\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "850d1b84-b248-4472-a027-97c206f1c718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog'],\n",
       "  'jumps'),\n",
       " (['the', 'fast', 'runner', 'easily', 'overtook', 'his', 'competitor'],\n",
       "  'overtakes')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Підготовка даних у форматі контексту та цільового слова\n",
    "training_data = [(context.split(), target) for context, target in data]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10420f51-c447-4f3d-bc90-9fcf7199f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embedding layer для конвертації індексів слів у векторні представлення\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.context = nn.Linear(embed_size, vocab_size)  # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "    def forward(self, inputs):  # Вхід: тензор індексів слів [TENSOR]\n",
    "        embeds = self.embedding(inputs)  # Отримання векторного представлення для кожного слова в контексті [TENSOR, EMBED_SIZE]\n",
    "        embeds_mean = torch.mean(embeds, dim=0)  # Обчислення середнього вектора для всього контексту [EMBED_SIZE]\n",
    "        out = self.context(embeds_mean)  # Передача середнього вектора через контекстний шар [VOCAB_SIZE]\n",
    "        log_probs = F.log_softmax(out, dim=0)  # Обчислення логарифмів ймовірностей для кожного слова словника [VOCAB_SIZE]\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eab8506-d10a-432c-a83e-18d6df97a906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.063270807266235\n",
      "Epoch 2, Loss: 5.001727104187012\n",
      "Epoch 3, Loss: 4.944370269775391\n",
      "Epoch 4, Loss: 4.88770055770874\n",
      "Epoch 5, Loss: 4.831376552581787\n",
      "Epoch 6, Loss: 4.775300979614258\n",
      "Epoch 7, Loss: 4.719436883926392\n",
      "Epoch 8, Loss: 4.663767099380493\n",
      "Epoch 9, Loss: 4.608281373977661\n",
      "Epoch 10, Loss: 4.552973985671997\n",
      "Epoch 11, Loss: 4.497838973999023\n",
      "Epoch 12, Loss: 4.442871570587158\n",
      "Epoch 13, Loss: 4.388066530227661\n",
      "Epoch 14, Loss: 4.333418607711792\n",
      "Epoch 15, Loss: 4.2789223194122314\n",
      "Epoch 16, Loss: 4.224572777748108\n",
      "Epoch 17, Loss: 4.1703643798828125\n",
      "Epoch 18, Loss: 4.116292953491211\n",
      "Epoch 19, Loss: 4.062353610992432\n",
      "Epoch 20, Loss: 4.008542776107788\n",
      "Epoch 21, Loss: 3.954857349395752\n",
      "Epoch 22, Loss: 3.9012949466705322\n",
      "Epoch 23, Loss: 3.847854256629944\n",
      "Epoch 24, Loss: 3.794535279273987\n",
      "Epoch 25, Loss: 3.7413374185562134\n",
      "Epoch 26, Loss: 3.688263416290283\n",
      "Epoch 27, Loss: 3.6353152990341187\n",
      "Epoch 28, Loss: 3.5824965238571167\n",
      "Epoch 29, Loss: 3.529811978340149\n",
      "Epoch 30, Loss: 3.4772671461105347\n",
      "Epoch 31, Loss: 3.42486834526062\n",
      "Epoch 32, Loss: 3.372623562812805\n",
      "Epoch 33, Loss: 3.320541024208069\n",
      "Epoch 34, Loss: 3.2686299085617065\n",
      "Epoch 35, Loss: 3.216900587081909\n",
      "Epoch 36, Loss: 3.1653642654418945\n",
      "Epoch 37, Loss: 3.114032506942749\n",
      "Epoch 38, Loss: 3.0629178285598755\n",
      "Epoch 39, Loss: 3.012033462524414\n",
      "Epoch 40, Loss: 2.9613932371139526\n",
      "Epoch 41, Loss: 2.9110119342803955\n",
      "Epoch 42, Loss: 2.8609038591384888\n",
      "Epoch 43, Loss: 2.811084508895874\n",
      "Epoch 44, Loss: 2.761569380760193\n",
      "Epoch 45, Loss: 2.7123748064041138\n",
      "Epoch 46, Loss: 2.6635162830352783\n",
      "Epoch 47, Loss: 2.6150104999542236\n",
      "Epoch 48, Loss: 2.566874146461487\n",
      "Epoch 49, Loss: 2.5191227197647095\n",
      "Epoch 50, Loss: 2.4717735052108765\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 32  # Розмір векторного представлення слів\n",
    "LEARNING_RATE = 0.001  # Швидкість навчання моделі\n",
    "EPOCHS = 50  # Кількість епох навчання\n",
    "\n",
    "torch.manual_seed(42)  # Фіксація seed для відтворюваності результатів\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM)  # Створення моделі CBOW з відповідними розмірами входу та виходу\n",
    "\n",
    "# Визначення функції втрат. Використовується NLLLoss, оскільки модель повертає логарифми ймовірностей\n",
    "loss_function = nn.NLLLoss()  # https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "\n",
    "# Визначення оптимізатора для навчання моделі (Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Цикл навчання моделі протягом заданої кількості епох\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    # Проходимося по кожному запису в тренувальних даних\n",
    "    for context, target in training_data:\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)  # Конвертація слова у відповідний індекс\n",
    "        target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)  # Конвертація цільового слова у відповідний індекс\n",
    "        \n",
    "        model.zero_grad()  # Обнулення градієнтів\n",
    "        \n",
    "        log_probs = model(context_idxs)  # Передача контексту через модель для отримання логарифмів ймовірностей\n",
    "        \n",
    "        # Обчислення втрат та градієнтів для оновлення параметрів моделі\n",
    "        loss = loss_function(log_probs.view(1, -1), target_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Додавання втрат для відстеження загальної втрати у кожній епохі\n",
    "\n",
    "    # Виведення втрат на кінець кожної епохи/\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf26ade1-16b2-4548-aaa8-b676aa7ecaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "tensor([-1.4690, -0.7932, -0.3008,  1.6401,  0.3698, -0.5005,  0.2208, -0.7009,\n",
      "        -1.6204,  1.0753, -0.9587, -0.6892, -1.3657,  2.2028, -1.2310, -0.4128,\n",
      "        -0.9899, -0.7467,  0.0025,  0.4714, -0.4732,  1.1207, -0.7514, -0.6439,\n",
      "        -1.4227,  0.0846, -0.1352,  0.6963, -0.0157,  1.8053, -1.2461,  1.3962],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Отримання векторного представлення слова\n",
    "word_emb = model.embedding(torch.tensor(word_to_ix[\"fox\"], dtype=torch.long))  # Подача індексу слова на вході до Embedding шару моделі\n",
    "\n",
    "print(word_emb.shape)  # Виведення розміру отриманого вектору\n",
    "print(word_emb)  # Виведення отриманого векторного представлення слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3be9cc81-42c5-4fb2-8472-35a457cbfe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: jumps\n"
     ]
    }
   ],
   "source": [
    "# Тестування моделі з контекстом для передбачення цільового слова\n",
    "context = [\"the\", \"quick\", \"brown\", \"fox\"]  # Контекст для передбачення цільового слова\n",
    "\n",
    "context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)  # Конвертація контексту у відповідні індекси\n",
    "\n",
    "log_probs = model(context_idxs)  # Передача контексту через модель для отримання логарифмів ймовірностей\n",
    "\n",
    "_, predicted_idx = torch.max(log_probs.view(1, -1), 1)  # Вибір індексу з найбільшим значенням ймовірності\n",
    "\n",
    "print(f'Predicted word: {vocab[predicted_idx]}')  # Виведення передбаченого слова"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a8344-49a2-4b72-8b13-b2ec8745da29",
   "metadata": {},
   "source": [
    "### Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c5816ab-6def-4038-b1d2-b1677b9fa5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'easily': 0,\n",
       " 'fox': 1,\n",
       " 'his': 2,\n",
       " 'jumped': 3,\n",
       " 'lazy': 4,\n",
       " 'brown': 5,\n",
       " 'the': 6,\n",
       " 'fast': 7,\n",
       " 'competitor': 8,\n",
       " 'quick': 9,\n",
       " 'over': 10,\n",
       " 'runner': 11,\n",
       " 'overtook': 12,\n",
       " 'dog': 13}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Використання простих даних\n",
    "data = [\n",
    "    \"the quick brown fox jumped over the lazy dog\",  # перше речення\n",
    "    \"the fast runner easily overtook his competitor\",  # друге речення\n",
    "    # Додавання додаткових речень, якщо потрібно\n",
    "]\n",
    "\n",
    "# Створення словника\n",
    "vocab = set()  # Ініціалізація пустого множинного словника\n",
    "for sentence in data:\n",
    "    vocab.update(sentence.split())  # Додавання слів з кожного речення у словник\n",
    "vocab = list(vocab)  # Перетворення множини у список, щоб мати можливість індексації\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}  # Створення словника індексів для кожного слова у словнику\n",
    "\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4667b5c7-6265-4026-a841-b27460f0fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size 52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 'quick'),\n",
       " ('the', 'brown'),\n",
       " ('quick', 'the'),\n",
       " ('quick', 'brown'),\n",
       " ('quick', 'fox'),\n",
       " ('brown', 'the'),\n",
       " ('brown', 'quick'),\n",
       " ('brown', 'fox'),\n",
       " ('brown', 'jumped'),\n",
       " ('fox', 'quick'),\n",
       " ('fox', 'brown'),\n",
       " ('fox', 'jumped'),\n",
       " ('fox', 'over'),\n",
       " ('jumped', 'brown'),\n",
       " ('jumped', 'fox'),\n",
       " ('jumped', 'over'),\n",
       " ('jumped', 'the'),\n",
       " ('over', 'fox'),\n",
       " ('over', 'jumped'),\n",
       " ('over', 'the')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Підготовка даних у форматі цільового слова та контексту\n",
    "WINDOW_SIZE = 2  # Розмір вікна контексту\n",
    "\n",
    "training_data = []  # Ініціалізація пустого списку тренувальних даних\n",
    "for sentence in data:\n",
    "    words = sentence.split()  # Розбиття речення на окремі слова\n",
    "    for i, target in enumerate(words):\n",
    "        context = [words[j] for j in range(max(0, i - WINDOW_SIZE), min(len(words), i + WINDOW_SIZE + 1)) if j != i]  # Формування контексту навколо цільового слова\n",
    "        for context_word in context:\n",
    "            training_data.append((target, context_word))  # Додавання пари (цільове слово, слово контексту) до тренувальних даних\n",
    "\n",
    "print(\"Training Size\", len(training_data))  # Виведення розміру тренувальних даних\n",
    "training_data[:20]  # Виведення перших 20 записів тренувальних даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f070598-4929-4d2e-91f9-af617a32844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # Embedding layer для конвертації індексів слів у векторні представлення\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Контекстний шар для перетворення векторного представлення цільового слова у розподіл ймовірностей над словами словника\n",
    "        self.context = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, target):\n",
    "        embeds = self.embedding(target)  # Отримання векторного представлення для цільового слова\n",
    "        out = self.context(embeds)  # Передача векторного представлення через контекстний шар\n",
    "        log_probs = F.log_softmax(out, dim=1)  # Обчислення логарифмів ймовірностей для кожного слова словника\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20295d32-9d63-4a78-8675-b509520df640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 143.06319761276245\n",
      "Epoch 2, Loss: 136.11760580539703\n",
      "Epoch 3, Loss: 131.4817761182785\n",
      "Epoch 4, Loss: 127.26307535171509\n",
      "Epoch 5, Loss: 123.41577744483948\n",
      "Epoch 6, Loss: 119.90144371986389\n",
      "Epoch 7, Loss: 116.68616664409637\n",
      "Epoch 8, Loss: 113.74019968509674\n",
      "Epoch 9, Loss: 111.03728175163269\n",
      "Epoch 10, Loss: 108.55406451225281\n",
      "Epoch 11, Loss: 106.26967394351959\n",
      "Epoch 12, Loss: 104.16538536548615\n",
      "Epoch 13, Loss: 102.22440135478973\n",
      "Epoch 14, Loss: 100.43163502216339\n",
      "Epoch 15, Loss: 98.77357745170593\n",
      "Epoch 16, Loss: 97.23813676834106\n",
      "Epoch 17, Loss: 95.81451308727264\n",
      "Epoch 18, Loss: 94.49306333065033\n",
      "Epoch 19, Loss: 93.2651858329773\n",
      "Epoch 20, Loss: 92.12319219112396\n",
      "Epoch 21, Loss: 91.06019186973572\n",
      "Epoch 22, Loss: 90.06998765468597\n",
      "Epoch 23, Loss: 89.14699423313141\n",
      "Epoch 24, Loss: 88.28614270687103\n",
      "Epoch 25, Loss: 87.48281341791153\n",
      "Epoch 26, Loss: 86.73278141021729\n",
      "Epoch 27, Loss: 86.03216683864594\n",
      "Epoch 28, Loss: 85.37738925218582\n",
      "Epoch 29, Loss: 84.76515185832977\n",
      "Epoch 30, Loss: 84.19239377975464\n",
      "Epoch 31, Loss: 83.65628772974014\n",
      "Epoch 32, Loss: 83.15420842170715\n",
      "Epoch 33, Loss: 82.68371939659119\n",
      "Epoch 34, Loss: 82.24255907535553\n",
      "Epoch 35, Loss: 81.82863473892212\n",
      "Epoch 36, Loss: 81.43999737501144\n",
      "Epoch 37, Loss: 81.0748479962349\n",
      "Epoch 38, Loss: 80.73150676488876\n",
      "Epoch 39, Loss: 80.40842843055725\n",
      "Epoch 40, Loss: 80.10417819023132\n",
      "Epoch 41, Loss: 79.8174215555191\n",
      "Epoch 42, Loss: 79.54693287611008\n",
      "Epoch 43, Loss: 79.29156929254532\n",
      "Epoch 44, Loss: 79.05028194189072\n",
      "Epoch 45, Loss: 78.8220944404602\n",
      "Epoch 46, Loss: 78.6061082482338\n",
      "Epoch 47, Loss: 78.4015007019043\n",
      "Epoch 48, Loss: 78.2074938416481\n",
      "Epoch 49, Loss: 78.02338701486588\n",
      "Epoch 50, Loss: 77.84852623939514\n",
      "Epoch 51, Loss: 77.68230432271957\n",
      "Epoch 52, Loss: 77.52416455745697\n",
      "Epoch 53, Loss: 77.37359309196472\n",
      "Epoch 54, Loss: 77.23010808229446\n",
      "Epoch 55, Loss: 77.09327203035355\n",
      "Epoch 56, Loss: 76.96267586946487\n",
      "Epoch 57, Loss: 76.837941467762\n",
      "Epoch 58, Loss: 76.71871775388718\n",
      "Epoch 59, Loss: 76.6046774983406\n",
      "Epoch 60, Loss: 76.4955205321312\n",
      "Epoch 61, Loss: 76.39096397161484\n",
      "Epoch 62, Loss: 76.29075247049332\n",
      "Epoch 63, Loss: 76.19463521242142\n",
      "Epoch 64, Loss: 76.10239237546921\n",
      "Epoch 65, Loss: 76.0138087272644\n",
      "Epoch 66, Loss: 75.92868834733963\n",
      "Epoch 67, Loss: 75.84684711694717\n",
      "Epoch 68, Loss: 75.76811420917511\n",
      "Epoch 69, Loss: 75.69232678413391\n",
      "Epoch 70, Loss: 75.61933755874634\n",
      "Epoch 71, Loss: 75.54900240898132\n",
      "Epoch 72, Loss: 75.48119193315506\n",
      "Epoch 73, Loss: 75.4157800078392\n",
      "Epoch 74, Loss: 75.35265219211578\n",
      "Epoch 75, Loss: 75.29169660806656\n",
      "Epoch 76, Loss: 75.23281210660934\n",
      "Epoch 77, Loss: 75.1758993268013\n",
      "Epoch 78, Loss: 75.12086933851242\n",
      "Epoch 79, Loss: 75.06763911247253\n",
      "Epoch 80, Loss: 75.01612317562103\n",
      "Epoch 81, Loss: 74.96624493598938\n",
      "Epoch 82, Loss: 74.917933344841\n",
      "Epoch 83, Loss: 74.87112122774124\n",
      "Epoch 84, Loss: 74.82574141025543\n",
      "Epoch 85, Loss: 74.78173446655273\n",
      "Epoch 86, Loss: 74.73904871940613\n",
      "Epoch 87, Loss: 74.69761770963669\n",
      "Epoch 88, Loss: 74.65739792585373\n",
      "Epoch 89, Loss: 74.61833590269089\n",
      "Epoch 90, Loss: 74.58038866519928\n",
      "Epoch 91, Loss: 74.5435106754303\n",
      "Epoch 92, Loss: 74.50765877962112\n",
      "Epoch 93, Loss: 74.47279644012451\n",
      "Epoch 94, Loss: 74.43887859582901\n",
      "Epoch 95, Loss: 74.40587747097015\n",
      "Epoch 96, Loss: 74.37375342845917\n",
      "Epoch 97, Loss: 74.34247374534607\n",
      "Epoch 98, Loss: 74.3120127916336\n",
      "Epoch 99, Loss: 74.28233587741852\n",
      "Epoch 100, Loss: 74.25341093540192\n",
      "Epoch 101, Loss: 74.22521930932999\n",
      "Epoch 102, Loss: 74.1977310180664\n",
      "Epoch 103, Loss: 74.17092382907867\n",
      "Epoch 104, Loss: 74.14476835727692\n",
      "Epoch 105, Loss: 74.11924862861633\n",
      "Epoch 106, Loss: 74.09433847665787\n",
      "Epoch 107, Loss: 74.07001912593842\n",
      "Epoch 108, Loss: 74.04627108573914\n",
      "Epoch 109, Loss: 74.02307760715485\n",
      "Epoch 110, Loss: 74.0004124045372\n",
      "Epoch 111, Loss: 73.97827231884003\n",
      "Epoch 112, Loss: 73.95662581920624\n",
      "Epoch 113, Loss: 73.93546795845032\n",
      "Epoch 114, Loss: 73.91477715969086\n",
      "Epoch 115, Loss: 73.89454072713852\n",
      "Epoch 116, Loss: 73.87474459409714\n",
      "Epoch 117, Loss: 73.85537540912628\n",
      "Epoch 118, Loss: 73.83641940355301\n",
      "Epoch 119, Loss: 73.81786584854126\n",
      "Epoch 120, Loss: 73.79970014095306\n",
      "Epoch 121, Loss: 73.78191471099854\n",
      "Epoch 122, Loss: 73.76449221372604\n",
      "Epoch 123, Loss: 73.74742662906647\n",
      "Epoch 124, Loss: 73.73070871829987\n",
      "Epoch 125, Loss: 73.71432304382324\n",
      "Epoch 126, Loss: 73.69826394319534\n",
      "Epoch 127, Loss: 73.6825265288353\n",
      "Epoch 128, Loss: 73.66709458827972\n",
      "Epoch 129, Loss: 73.65196001529694\n",
      "Epoch 130, Loss: 73.63711875677109\n",
      "Epoch 131, Loss: 73.62255847454071\n",
      "Epoch 132, Loss: 73.60827487707138\n",
      "Epoch 133, Loss: 73.59426194429398\n",
      "Epoch 134, Loss: 73.58050847053528\n",
      "Epoch 135, Loss: 73.56700587272644\n",
      "Epoch 136, Loss: 73.55375528335571\n",
      "Epoch 137, Loss: 73.54074507951736\n",
      "Epoch 138, Loss: 73.52796918153763\n",
      "Epoch 139, Loss: 73.515420794487\n",
      "Epoch 140, Loss: 73.5030956864357\n",
      "Epoch 141, Loss: 73.49098825454712\n",
      "Epoch 142, Loss: 73.4790934920311\n",
      "Epoch 143, Loss: 73.46740275621414\n",
      "Epoch 144, Loss: 73.45591670274734\n",
      "Epoch 145, Loss: 73.44462621212006\n",
      "Epoch 146, Loss: 73.4335263967514\n",
      "Epoch 147, Loss: 73.42261379957199\n",
      "Epoch 148, Loss: 73.41188472509384\n",
      "Epoch 149, Loss: 73.40133380889893\n",
      "Epoch 150, Loss: 73.39095813035965\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 32  # Розмір векторного представлення слів\n",
    "LEARNING_RATE = 0.001  # Швидкість навчання моделі\n",
    "EPOCHS = 150  # Кількість епох навчання\n",
    "\n",
    "torch.manual_seed(42)  # Фіксація seed для відтворюваності результатів\n",
    "model = SkipGram(len(vocab), EMBEDDING_DIM)  # Створення моделі SkipGram з відповідними розмірами входу та виходу\n",
    "loss_function = nn.NLLLoss()  # Визначення функції втрат. Використовується NLLLoss, оскільки модель повертає логарифми ймовірностей\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Визначення оптимізатора для навчання моделі (Adam)\n",
    "\n",
    "# Цикл навчання моделі протягом заданої кількості епох\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    # Проходимося по кожній парі цільового слова та контексту у тренувальних даних\n",
    "    for target, context in training_data:\n",
    "        target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)  # Конвертація цільового слова у відповідний індекс\n",
    "        context_idx = torch.tensor([word_to_ix[context]], dtype=torch.long)  # Конвертація слова контексту у відповідний індекс\n",
    "        \n",
    "        model.zero_grad()  # Обнулення градієнтів\n",
    "        \n",
    "        log_probs = model(target_idx)  # Передача цільового слова через модель для отримання логарифмів ймовірностей\n",
    "        \n",
    "        # Обчислення втрат та градієнтів для оновлення параметрів моделі\n",
    "        loss = loss_function(log_probs, context_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()  # Додавання втрат для відстеження загальної втрати у кожній епохі\n",
    "\n",
    "    # Виведення втрат на кінець кожної епохи\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05a4737c-97c9-4af6-99f5-96467573030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4587, -0.9210,  0.2019,  1.9399,  0.4100, -0.1573,  0.6388, -0.8298,\n",
      "        -1.8157,  1.0839, -0.9182, -0.6527, -1.4537,  2.7578, -1.2844, -0.5320,\n",
      "        -0.9350, -0.7515,  0.1263,  0.3258, -0.7514,  1.1269, -1.3218, -0.9853,\n",
      "        -1.6952, -0.0351, -0.2127,  0.2832, -0.2839,  2.2281, -1.2435,  1.4269],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Отримання векторного представлення слова\n",
    "word_emb = model.embedding(torch.tensor(word_to_ix[\"fox\"], dtype=torch.long))  # Подача індексу слова на вході до Embedding шару моделі\n",
    "print(word_emb)  # Виведення векторного представлення слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "589c911a-ee71-45de-b9b8-23a28d503cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted context word: dog\n"
     ]
    }
   ],
   "source": [
    "# Тестування моделі з цільовим словом для передбачення слов контексту\n",
    "target = \"lazy\"  # Цільове слово для передбачення контексту\n",
    "target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)  # Конвертація цільового слова у відповідний індекс\n",
    "log_probs = model(target_idx)  # Передача цільового слова через модель для отримання логарифмів ймовірностей\n",
    "_, predicted_idx = torch.max(log_probs, 1)  # Вибір індексу з найбільшим значенням ймовірності\n",
    "print(f'Predicted context word: {vocab[predicted_idx]}')  # Виведення передбаченого слова контексту"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e76613-cbab-4236-9d22-8490bb6df8c7",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** — це метод отримання векторних представлень слів, аналогічний Word2Vec. Розроблений Френсісом Янгом, Крістіаном Мендельсоном та Іксін Лі у 2014 році, GloVe відрізняється своєю концепцією, що відрізняється від інших методів.\n",
    "\n",
    "Основна ідея GloVe полягає у використанні глобальної статистики корпусу мови для отримання векторних представлень слів. Він використовує матрицю зважувань, яка представляє взаємозв'язки слів у корпусі, і намагається вивчити такі вектори, які максимально точно відображають ці статистичні відносини.\n",
    "\n",
    "Основні принципи GloVe:\n",
    "1. **Матриця зважувань:** GloVe використовує матрицю зважувань, яка містить інформацію про взаємозв'язки слів у корпусі мови.\n",
    "2. **Глобальна статистика:** Використання глобальних статистик дозволяє моделі враховувати глобальні структури мови та семантичні відносини.\n",
    "3. **Оптимізація функції втрат:** Метод оптимізує функцію втрат для знаходження векторів слів, які найкраще відображають глобальні зв'язки між словами.\n",
    "\n",
    "GloVe надає досить ефективні векторні представлення слів, особливо коли у корпусі є велика кількість тексту та статистика взаємозв'язків слів є значущою. Він часто використовується у задачах обробки природної мови та машинного навчання для отримання семантично багатозначних представлень слів.\n",
    "\n",
    "### Step 1: Building the Co-occurrence Matrix\n",
    "\n",
    "Дано корпус, обчисліть матрицю спільного входження $X$, де $X_{ij}$​ представляє кількість повторень слова $j$ зустрічається в контексті слова $i$. Контекст можна визначити як вікно навколо слова $i$.\n",
    "\n",
    "### Step 2: Matrix Factorization\n",
    "\n",
    "Основна ідея GloVe полягає в тому, щоб розкласти на множники матрицю спільного входження, щоб знайти векторне представлення слів. Він спрямований на вивчення таких векторів слів, що їх скалярний добуток дорівнює логарифму ймовірності спільного зустрічання слів.\n",
    "\n",
    "Цільова функція визначається як:\n",
    "\n",
    "$$ J = \\sum_{i,j=1}^V f(X_{ij})(w_i^T \\hat{w}_j + b_i + \\hat{b}_j - \\log(X_{ij}))^2 $$\n",
    "\n",
    "Де:\n",
    "- $w_j$ і $\\hat{w}_j$ є векторами слів для слів $i$ і $j$ відповідно.\n",
    "- $b_i$ і $\\hat{b}_j$ є упередженими термінами для слів $i$ і $j$ відповідно.\n",
    "- $f(X_{ij})$ це вагова функція, яка призначає відносно меншу вагу рідкісним і частим подіям.\n",
    "- $V$ це розмір словникового запасу.\n",
    "\n",
    "### Step 3: Weighting Function\n",
    "\n",
    "Вагова функція $f(x)$ вибрано для того, щоб запобігти домінуванню надто частих пар слів у навчанні. Це визначається як:\n",
    "\n",
    "$$f(x) = \\begin{cases} (\\frac{x}{x_{\\max}})^{\\alpha} & \\textit{if } x < x_{\\max} \\\\ 1 & \\textit{otherwise} \\end{cases}$$\n",
    "\n",
    "Зазвичай, $\\alpha=0.75$ and $x_{\\max}=100$.\n",
    "\n",
    "### Step 4: Training\n",
    "\n",
    "1. **Initialization:** Випадково ініціалізуйте вектори слів і умови зміщення (bias).\n",
    "3. **Optimization:** Використовуйте градієнтний спуск, щоб мінімізувати цільову функцію $J$, оновлюючи вектори слів і умови зсуву, щоб зменшити вартість/функцію втрат (cost function).\n",
    "4. **Iterations:** Перебирайте кожну пару слів у корпусі та оновлюйте вектори, щоб мінімізувати витрати, пов’язані з кожною парою.\n",
    "\n",
    "Після навчання $w$ або $\\hat{w}$ можна використовувати як вектори слів або їх можна комбінувати.\n",
    "\n",
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c96be86b-22bf-4ef3-b4e6-5604a78bd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  # Імпорт класу Counter для підрахунку елементів у списку\n",
    "from scipy.sparse import coo_matrix  # Імпорт функції coo_matrix для створення розрідженої матриці\n",
    "import numpy as np  # Імпорт бібліотеки NumPy для роботи з масивами та матрицями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "027a9e78-5859-4080-aef6-129f3a60a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fox': 0, 'jumped': 1, 'lazy': 2, 'brown': 3, 'the': 4, 'quick': 5, 'over': 6, 'dog': 7}\n"
     ]
    }
   ],
   "source": [
    "def create_cooccurrence_matrix(corpus, vocab, window_size=2):\n",
    "    vocab_size = len(vocab)  # Визначення розміру словника\n",
    "    word2id = {word: i for i, word in enumerate(vocab)}  # Створення словника індексів для слів у словнику\n",
    "    \n",
    "    cooccurrences = Counter()  # Ініціалізація лічильника для підрахунку співвходжень слів\n",
    "    \n",
    "    # Проходимося по кожному реченню у корпусі\n",
    "    for sentence in corpus:\n",
    "        sentence = [word for word in sentence if word in vocab]  # Вибірка лише слів, які присутні у словнику\n",
    "        # Проходимося по кожному слову у реченні\n",
    "        for i, word in enumerate(sentence):\n",
    "            # Проходимося по словам у вікні навколо поточного слова\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    # Збільшення лічильника співвходжень між поточним словом та словом у вікні\n",
    "                    cooccurrences[(word2id[word], word2id[sentence[j]])] += 1\n",
    "    \n",
    "    # Розпакування індексів та значень лічильника, щоб сформувати розріджену матрицю\n",
    "    ij_indices, values = zip(*cooccurrences.most_common())\n",
    "    i_indices, j_indices = zip(*ij_indices)\n",
    "\n",
    "    print(word2id)\n",
    "    \n",
    "    # Створення розрідженої матриці з отриманих індексів та значень\n",
    "    cooccurrence_matrix = coo_matrix((values, (i_indices, j_indices)), shape=(vocab_size, vocab_size))\n",
    "    \n",
    "    return cooccurrence_matrix\n",
    "\n",
    "# Визначення корпусу та словника\n",
    "corpus = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
    "vocab = list(set(word for sentence in corpus for word in sentence))\n",
    "\n",
    "# Створення матриці співвходжень\n",
    "X_ij = create_cooccurrence_matrix(corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff1635a7-c8ab-4b0a-8e69-4e75a3c45981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54d2e97a-40fb-409f-bd4e-f562e963bc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac2a0827-5702-48ff-9e6d-6200e8d98668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        # Embedding шар для векторного представлення слів\n",
    "        self.w_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.w_dash_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Embedding шар для зміщень\n",
    "        self.w_biases = nn.Embedding(vocab_size, 1)\n",
    "        self.w_dash_biases = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, i_indices, j_indices):\n",
    "        # Отримання векторних представлень слів за їхніми індексами\n",
    "        w_embeds = self.w_embeddings(i_indices)\n",
    "        w_dash_embeds = self.w_dash_embeddings(j_indices)\n",
    "        \n",
    "        # Отримання зміщень за індексами\n",
    "        w_bias = self.w_biases(i_indices)\n",
    "        w_dash_bias = self.w_dash_biases(j_indices)\n",
    "        \n",
    "        # Обчислення ваги (x_ij) для кожної пари (i, j)\n",
    "        x = (torch.sum(w_embeds * w_dash_embeds, dim=1) + w_bias.squeeze() + w_dash_bias.squeeze())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87a6e7de-2f31-4c3e-bf67-52638abc81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_loss(x, x_max, alpha, y):\n",
    "    # Обчислення ваги для кожного значення x за допомогою функції ваг\n",
    "    weight = torch.where(x < x_max, (x/x_max) ** alpha, torch.ones_like(x))  # f(x)\n",
    "    # Обчислення втрат GloVe\n",
    "    return torch.sum(weight * (y - torch.log(x)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21f76095-1136-4871-aab0-644d738504c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.779045104980469\n",
      "Epoch 2, Loss: 4.755053997039795\n",
      "Epoch 3, Loss: 4.731152534484863\n",
      "Epoch 4, Loss: 4.707340240478516\n",
      "Epoch 5, Loss: 4.683619499206543\n",
      "Epoch 6, Loss: 4.6599907875061035\n",
      "Epoch 7, Loss: 4.636454105377197\n",
      "Epoch 8, Loss: 4.613012313842773\n",
      "Epoch 9, Loss: 4.58966588973999\n",
      "Epoch 10, Loss: 4.566415309906006\n",
      "Epoch 11, Loss: 4.543262481689453\n",
      "Epoch 12, Loss: 4.520206928253174\n",
      "Epoch 13, Loss: 4.497249603271484\n",
      "Epoch 14, Loss: 4.474391937255859\n",
      "Epoch 15, Loss: 4.451634883880615\n",
      "Epoch 16, Loss: 4.428977012634277\n",
      "Epoch 17, Loss: 4.406421184539795\n",
      "Epoch 18, Loss: 4.38396692276001\n",
      "Epoch 19, Loss: 4.361615180969238\n",
      "Epoch 20, Loss: 4.3393659591674805\n",
      "Epoch 21, Loss: 4.317220211029053\n",
      "Epoch 22, Loss: 4.295177936553955\n",
      "Epoch 23, Loss: 4.2732391357421875\n",
      "Epoch 24, Loss: 4.251404285430908\n",
      "Epoch 25, Loss: 4.229674339294434\n",
      "Epoch 26, Loss: 4.208048343658447\n",
      "Epoch 27, Loss: 4.186528205871582\n",
      "Epoch 28, Loss: 4.165112495422363\n",
      "Epoch 29, Loss: 4.143801689147949\n",
      "Epoch 30, Loss: 4.122596740722656\n",
      "Epoch 31, Loss: 4.101496696472168\n",
      "Epoch 32, Loss: 4.080502033233643\n",
      "Epoch 33, Loss: 4.059613227844238\n",
      "Epoch 34, Loss: 4.038829326629639\n",
      "Epoch 35, Loss: 4.018150806427002\n",
      "Epoch 36, Loss: 3.997577667236328\n",
      "Epoch 37, Loss: 3.977108955383301\n",
      "Epoch 38, Loss: 3.9567441940307617\n",
      "Epoch 39, Loss: 3.9364852905273438\n",
      "Epoch 40, Loss: 3.916330099105835\n",
      "Epoch 41, Loss: 3.896279811859131\n",
      "Epoch 42, Loss: 3.8763322830200195\n",
      "Epoch 43, Loss: 3.8564882278442383\n",
      "Epoch 44, Loss: 3.8367481231689453\n",
      "Epoch 45, Loss: 3.817110538482666\n",
      "Epoch 46, Loss: 3.7975757122039795\n",
      "Epoch 47, Loss: 3.7781429290771484\n",
      "Epoch 48, Loss: 3.7588112354278564\n",
      "Epoch 49, Loss: 3.73958158493042\n",
      "Epoch 50, Loss: 3.7204530239105225\n"
     ]
    }
   ],
   "source": [
    "embed_size = 5  # Розмір векторного представлення слів\n",
    "x_max = 100  # Максимальне значення x\n",
    "alpha = 0.75  # Параметр alpha\n",
    "epochs = 50  # Кількість епох навчання\n",
    "lr = 1e-3  # Швидкість навчання\n",
    "\n",
    "torch.manual_seed(42)  # Фіксація seed для відтворюваності результатів\n",
    "model = GloVe(len(vocab), embed_size)  # Створення моделі GloVe з відповідними розмірами входу та виходу\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Визначення оптимізатора для навчання моделі (Adam)\n",
    "\n",
    "# Цикл навчання моделі протягом заданої кількості епох\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Обнулення градієнтів\n",
    "    \n",
    "    # Отримання індексів та значень з розрідженої матриці X_ij\n",
    "    i_indices, j_indices, values = X_ij.row, X_ij.col, X_ij.data\n",
    "    i_indices, j_indices, values = map(torch.LongTensor, (i_indices, j_indices, values))\n",
    "    \n",
    "    # Передача індексів та значень через модель для отримання вихідних значень\n",
    "    outputs = model(i_indices, j_indices)\n",
    "    # Обчислення втрат за допомогою функції glove_loss\n",
    "    loss = glove_loss(values.float(), x_max, alpha, outputs)\n",
    "    \n",
    "    loss.backward()  # Обчислення градієнтів\n",
    "    optimizer.step()  # Оновлення параметрів моделі\n",
    "    \n",
    "    # Виведення втрат на кінець кожної епохи\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "47abeb48-abab-4141-b152-43c60d0135b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7094,  1.0294,  0.7520,  1.6353,  0.0911],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_embeddings(torch.tensor(word_to_ix[\"lazy\"], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8e3cb-6599-4520-b29f-aeec72016327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
